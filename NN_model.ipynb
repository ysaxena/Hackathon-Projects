{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34224, 13) (79853, 14) (114077, 14)\n",
      "(34224, 19) (79853, 19) (114077, 19)\n",
      "(63882, 16) (63882, 1) (34224, 16) (15971, 16) 0.0020637023882889775\n",
      "Epoch 0: Loss 0.19358655570946515\n",
      "Epoch 1: Loss 0.19030158936801014\n",
      "Epoch 2: Loss 0.1889567846723483\n",
      "Epoch 3: Loss 0.1880497500619558\n",
      "Epoch 4: Loss 0.18750650169764588\n",
      "Epoch 5: Loss 0.18711469784589285\n",
      "Epoch 6: Loss 0.18678348942337444\n",
      "Epoch 7: Loss 0.18651525985894218\n",
      "Epoch 8: Loss 0.18629175194648417\n",
      "Epoch 9: Loss 0.18609571893128354\n",
      "lr 0.009000000000000001\n",
      "Epoch 10: Loss 0.18593216855711808\n",
      "Epoch 11: Loss 0.18581805296459047\n",
      "Epoch 12: Loss 0.1856757868954222\n",
      "Epoch 13: Loss 0.18555472368560305\n",
      "Epoch 14: Loss 0.18543647212475917\n",
      "Epoch 15: Loss 0.18533596405958924\n",
      "Epoch 16: Loss 0.18524326323629828\n",
      "Epoch 17: Loss 0.1851655401220407\n",
      "Epoch 18: Loss 0.18509620110639002\n",
      "Epoch 19: Loss 0.18504113557954732\n",
      "lr 0.008100000000000001\n",
      "Epoch 20: Loss 0.1849686662609264\n",
      "Epoch 21: Loss 0.1849087334902593\n",
      "Epoch 22: Loss 0.18484132664919697\n",
      "Epoch 23: Loss 0.18477954256341936\n",
      "Epoch 24: Loss 0.18471938138211777\n",
      "Epoch 25: Loss 0.18465585830257908\n",
      "Epoch 26: Loss 0.18460505289527937\n",
      "Epoch 27: Loss 0.18456032896106314\n",
      "Epoch 28: Loss 0.1845101172682862\n",
      "Epoch 29: Loss 0.18446980983064912\n",
      "lr 0.007290000000000001\n",
      "Epoch 30: Loss 0.18442392640073008\n",
      "Epoch 31: Loss 0.18438644182747804\n",
      "Epoch 32: Loss 0.18434939119789398\n",
      "Epoch 33: Loss 0.18430902308877306\n",
      "Epoch 34: Loss 0.18427583354050941\n",
      "Epoch 35: Loss 0.18423963856300207\n",
      "Epoch 36: Loss 0.18421226244867037\n",
      "Epoch 37: Loss 0.18417393728435888\n",
      "Epoch 38: Loss 0.1841412760500805\n",
      "Epoch 39: Loss 0.1841161933372484\n",
      "lr 0.006561000000000002\n",
      "Epoch 40: Loss 0.18408410887304608\n",
      "Epoch 41: Loss 0.18405541742310685\n",
      "Epoch 42: Loss 0.18402934929807338\n",
      "Epoch 43: Loss 0.18400284479412968\n",
      "Epoch 44: Loss 0.18398140977195684\n",
      "Epoch 45: Loss 0.18395348314334511\n",
      "Epoch 46: Loss 0.1839255279936264\n",
      "Epoch 47: Loss 0.1838998983934697\n",
      "Epoch 48: Loss 0.1838771714374693\n",
      "Epoch 49: Loss 0.18385335993811205\n",
      "lr 0.005904900000000002\n",
      "Epoch 50: Loss 0.18382737119421755\n",
      "Epoch 51: Loss 0.18380227336701374\n",
      "Epoch 52: Loss 0.18378014991030459\n",
      "Epoch 53: Loss 0.18375811160454697\n",
      "Epoch 54: Loss 0.18374136758034706\n",
      "Epoch 55: Loss 0.18371496835206333\n",
      "Epoch 56: Loss 0.18369085751203115\n",
      "Epoch 57: Loss 0.1836723112640217\n",
      "Epoch 58: Loss 0.1836508975935506\n",
      "Epoch 59: Loss 0.18363249397417625\n",
      "lr 0.005314410000000002\n",
      "Epoch 60: Loss 0.1836101679890668\n",
      "Epoch 61: Loss 0.18359208720399295\n",
      "Epoch 62: Loss 0.18357333302071915\n",
      "Epoch 63: Loss 0.1835550110268096\n",
      "Epoch 64: Loss 0.18353568930461708\n",
      "Epoch 65: Loss 0.18351464157074798\n",
      "Epoch 66: Loss 0.18349499489966767\n",
      "Epoch 67: Loss 0.18347525331196385\n",
      "Epoch 68: Loss 0.18345718855150375\n",
      "Epoch 69: Loss 0.18344147149058826\n",
      "lr 0.004782969000000002\n",
      "Epoch 70: Loss 0.1834260443037656\n",
      "Epoch 71: Loss 0.18340786627724867\n",
      "Epoch 72: Loss 0.183389716961385\n",
      "Epoch 73: Loss 0.18337262954216815\n",
      "Epoch 74: Loss 0.18335593301581396\n",
      "Epoch 75: Loss 0.1833381371222987\n",
      "Epoch 76: Loss 0.18332192860641894\n",
      "Epoch 77: Loss 0.18330638361566193\n",
      "Epoch 78: Loss 0.1832925125154867\n",
      "Epoch 79: Loss 0.1832773467834842\n",
      "lr 0.004304672100000002\n",
      "Epoch 80: Loss 0.18326212897367591\n",
      "Epoch 81: Loss 0.18324720226567143\n",
      "Epoch 82: Loss 0.18323267131499127\n",
      "Epoch 83: Loss 0.18321951984809187\n",
      "Epoch 84: Loss 0.1832049781183769\n",
      "Epoch 85: Loss 0.18319045986148402\n",
      "Epoch 86: Loss 0.18317663345999158\n",
      "Epoch 87: Loss 0.18316311976813593\n",
      "Epoch 88: Loss 0.18315101212692658\n",
      "Epoch 89: Loss 0.18313736028290548\n",
      "lr 0.003874204890000002\n",
      "Epoch 90: Loss 0.1831222162114668\n",
      "Epoch 91: Loss 0.18310861556172106\n",
      "Epoch 92: Loss 0.18309444870204264\n",
      "Epoch 93: Loss 0.1830817498174867\n",
      "Epoch 94: Loss 0.1830677764509205\n",
      "Epoch 95: Loss 0.1830551653247561\n",
      "Epoch 96: Loss 0.18304057904064802\n",
      "Epoch 97: Loss 0.18302777490911418\n",
      "Epoch 98: Loss 0.18301539037943945\n",
      "Epoch 99: Loss 0.18300273380084578\n",
      "lr 0.003486784401000002\n",
      "Epoch 100: Loss 0.1829891378622502\n",
      "Epoch 101: Loss 0.18297574472798953\n",
      "Epoch 102: Loss 0.1829625025116895\n",
      "Epoch 103: Loss 0.18294950857068787\n",
      "Epoch 104: Loss 0.18293738076090066\n",
      "Epoch 105: Loss 0.182923580151756\n",
      "Epoch 106: Loss 0.18290983713539932\n",
      "Epoch 107: Loss 0.18289684212012192\n",
      "Epoch 108: Loss 0.18288434873394496\n",
      "Epoch 109: Loss 0.1828717564656909\n",
      "lr 0.003138105960900002\n",
      "Epoch 110: Loss 0.18285894481938691\n",
      "Epoch 111: Loss 0.18284562231083767\n",
      "Epoch 112: Loss 0.18283231085337232\n",
      "Epoch 113: Loss 0.18281992930711485\n",
      "Epoch 114: Loss 0.1828082190609021\n",
      "Epoch 115: Loss 0.18279596955869526\n",
      "Epoch 116: Loss 0.18278555251557896\n",
      "Epoch 117: Loss 0.18277390339083013\n",
      "Epoch 118: Loss 0.18276215444072055\n",
      "Epoch 119: Loss 0.18275052408477258\n",
      "lr 0.0028242953648100018\n",
      "Epoch 120: Loss 0.18273776656903823\n",
      "Epoch 121: Loss 0.18272538777401423\n",
      "Epoch 122: Loss 0.1827141145804465\n",
      "Epoch 123: Loss 0.182701822556112\n",
      "Epoch 124: Loss 0.18269002289038697\n",
      "Epoch 125: Loss 0.18267809093784027\n",
      "Epoch 126: Loss 0.18266630776573653\n",
      "Epoch 127: Loss 0.18265464025450834\n",
      "Epoch 128: Loss 0.18264372179072258\n",
      "Epoch 129: Loss 0.18263322956525482\n",
      "lr 0.0025418658283290017\n",
      "Epoch 130: Loss 0.18262047971984105\n",
      "Epoch 131: Loss 0.18260973579115997\n",
      "Epoch 132: Loss 0.18259846408058053\n",
      "Epoch 133: Loss 0.18258731070968753\n",
      "Epoch 134: Loss 0.1825778726961686\n",
      "Epoch 135: Loss 0.182569431974312\n",
      "Epoch 136: Loss 0.1825591563511264\n",
      "Epoch 137: Loss 0.18254902833582218\n",
      "Epoch 138: Loss 0.18253870774022557\n",
      "Epoch 139: Loss 0.18252888827870573\n",
      "lr 0.0022876792454961017\n",
      "Epoch 140: Loss 0.18251813770814465\n",
      "Epoch 141: Loss 0.1825078615972335\n",
      "Epoch 142: Loss 0.1824979200207049\n",
      "Epoch 143: Loss 0.18248798399017188\n",
      "Epoch 144: Loss 0.18247831064003134\n",
      "Epoch 145: Loss 0.18247059347321887\n",
      "Epoch 146: Loss 0.1824608129421377\n",
      "Epoch 147: Loss 0.18245107070571687\n",
      "Epoch 148: Loss 0.18244144288558195\n",
      "Epoch 149: Loss 0.18243244639983242\n",
      "lr 0.0020589113209464917\n",
      "Epoch 150: Loss 0.18242262772581436\n",
      "Epoch 151: Loss 0.18241275544506402\n",
      "Epoch 152: Loss 0.18240313536963945\n",
      "Epoch 153: Loss 0.18239364261093347\n",
      "Epoch 154: Loss 0.18238420166708053\n",
      "Epoch 155: Loss 0.18237448758417504\n",
      "Epoch 156: Loss 0.1823654880157073\n",
      "Epoch 157: Loss 0.18235709657149282\n",
      "Epoch 158: Loss 0.18234789920975317\n",
      "Epoch 159: Loss 0.18233845891397554\n",
      "lr 0.0018530201888518425\n",
      "Epoch 160: Loss 0.18232925179390733\n",
      "Epoch 161: Loss 0.18232038507900006\n",
      "Epoch 162: Loss 0.18231205407854098\n",
      "Epoch 163: Loss 0.18230322799686277\n",
      "Epoch 164: Loss 0.18229451714424422\n",
      "Epoch 165: Loss 0.18228502915015724\n",
      "Epoch 166: Loss 0.1822765842360995\n",
      "Epoch 167: Loss 0.18226812823215877\n",
      "Epoch 168: Loss 0.18225947413912746\n",
      "Epoch 169: Loss 0.1822514172058286\n",
      "lr 0.0016677181699666583\n",
      "Epoch 170: Loss 0.1822425883374781\n",
      "Epoch 171: Loss 0.18223411478893084\n",
      "Epoch 172: Loss 0.1822256039835177\n",
      "Epoch 173: Loss 0.18221719724026694\n",
      "Epoch 174: Loss 0.18220930916096695\n",
      "Epoch 175: Loss 0.18220112317326384\n",
      "Epoch 176: Loss 0.1821926503346025\n",
      "Epoch 177: Loss 0.18218481800541336\n",
      "Epoch 178: Loss 0.1821769335813381\n",
      "Epoch 179: Loss 0.18216928446988548\n",
      "lr 0.0015009463529699924\n",
      "Epoch 180: Loss 0.18216149071953855\n",
      "Epoch 181: Loss 0.18215338620216776\n",
      "Epoch 182: Loss 0.18214523232224714\n",
      "Epoch 183: Loss 0.18213752036774059\n",
      "Epoch 184: Loss 0.18213081814087959\n",
      "Epoch 185: Loss 0.18212315612069566\n",
      "Epoch 186: Loss 0.18211558328035932\n",
      "Epoch 187: Loss 0.18210765453865657\n",
      "Epoch 188: Loss 0.18210032809842133\n",
      "Epoch 189: Loss 0.18209307340380962\n",
      "lr 0.0013508517176729932\n",
      "Epoch 190: Loss 0.18208518765040146\n",
      "Epoch 191: Loss 0.1820777919230006\n",
      "Epoch 192: Loss 0.18207066661995983\n",
      "Epoch 193: Loss 0.18206289178832927\n",
      "Epoch 194: Loss 0.18205482958458796\n",
      "Epoch 195: Loss 0.18204784768317528\n",
      "Epoch 196: Loss 0.18204055692270896\n",
      "Epoch 197: Loss 0.1820330684032638\n",
      "Epoch 198: Loss 0.18202644814139057\n",
      "Epoch 199: Loss 0.18201912359873212\n",
      "lr 0.001215766545905694\n",
      "Epoch 200: Loss 0.1820118564005205\n",
      "Epoch 201: Loss 0.18200462933420578\n",
      "Epoch 202: Loss 0.1819975090721285\n",
      "Epoch 203: Loss 0.18199019794926546\n",
      "Epoch 204: Loss 0.1819828946237231\n",
      "Epoch 205: Loss 0.18197589993310678\n",
      "Epoch 206: Loss 0.1819691173892884\n",
      "Epoch 207: Loss 0.18196219616955445\n",
      "Epoch 208: Loss 0.1819549277388534\n",
      "Epoch 209: Loss 0.18194838023512516\n",
      "lr 0.0010941898913151245\n",
      "Epoch 210: Loss 0.18194159377287678\n",
      "Epoch 211: Loss 0.18193489550712658\n",
      "Epoch 212: Loss 0.18192827009985119\n",
      "Epoch 213: Loss 0.18192120149462432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214: Loss 0.18191410903680447\n",
      "Epoch 215: Loss 0.18190741984256373\n",
      "Epoch 216: Loss 0.181900267824637\n",
      "Epoch 217: Loss 0.1818940166342275\n",
      "Epoch 218: Loss 0.18188748288727433\n",
      "Epoch 219: Loss 0.18188082521100224\n",
      "lr 0.0009847709021836122\n",
      "Epoch 220: Loss 0.18187365407529116\n",
      "Epoch 221: Loss 0.18186674894444313\n",
      "Epoch 222: Loss 0.18186010596709987\n",
      "Epoch 223: Loss 0.181853690875573\n",
      "Epoch 224: Loss 0.18184759225177266\n",
      "Epoch 225: Loss 0.1818413365709482\n",
      "Epoch 226: Loss 0.18183524323625075\n",
      "Epoch 227: Loss 0.1818290590712395\n",
      "Epoch 228: Loss 0.18182280989590696\n",
      "Epoch 229: Loss 0.18181686743375866\n",
      "lr 0.0008862938119652509\n",
      "Epoch 230: Loss 0.18181077280130173\n",
      "Epoch 231: Loss 0.18180435782063306\n",
      "Epoch 232: Loss 0.18179783917290462\n",
      "Epoch 233: Loss 0.18179172686583528\n",
      "Epoch 234: Loss 0.18178549014099674\n",
      "Epoch 235: Loss 0.18178015361239747\n",
      "Epoch 236: Loss 0.18177441820870494\n",
      "Epoch 237: Loss 0.18176820507863597\n",
      "Epoch 238: Loss 0.1817623035156699\n",
      "Epoch 239: Loss 0.18175653970471042\n",
      "lr 0.0007976644307687258\n",
      "Epoch 240: Loss 0.18175026648910525\n",
      "Epoch 241: Loss 0.18174446562883584\n",
      "Epoch 242: Loss 0.18173855431634917\n",
      "Epoch 243: Loss 0.1817329878993585\n",
      "Epoch 244: Loss 0.1817276558297478\n",
      "Epoch 245: Loss 0.1817219438797225\n",
      "Epoch 246: Loss 0.18171621788005254\n",
      "Epoch 247: Loss 0.18171067095117974\n",
      "Epoch 248: Loss 0.18170518695386842\n",
      "Epoch 249: Loss 0.18169962066537954\n",
      "lr 0.0007178979876918532\n",
      "Epoch 250: Loss 0.18169415276829795\n",
      "Epoch 251: Loss 0.18168826785763995\n",
      "Epoch 252: Loss 0.18168254706379852\n",
      "Epoch 253: Loss 0.18167726251885027\n",
      "Epoch 254: Loss 0.18167174787732238\n",
      "Epoch 255: Loss 0.18166636588628313\n",
      "Epoch 256: Loss 0.18166119177656992\n",
      "Epoch 257: Loss 0.1816558401487036\n",
      "Epoch 258: Loss 0.1816504575214758\n",
      "Epoch 259: Loss 0.1816453168808383\n",
      "lr 0.0006461081889226679\n",
      "Epoch 260: Loss 0.18164013187939715\n",
      "Epoch 261: Loss 0.18163467896149635\n",
      "Epoch 262: Loss 0.18162931504968152\n",
      "Epoch 263: Loss 0.18162387176578815\n",
      "Epoch 264: Loss 0.1816187337741198\n",
      "Epoch 265: Loss 0.18161361342333815\n",
      "Epoch 266: Loss 0.1816084255231948\n",
      "Epoch 267: Loss 0.18160342644152647\n",
      "Epoch 268: Loss 0.181598316019354\n",
      "Epoch 269: Loss 0.18159345984672692\n",
      "lr 0.0005814973700304011\n",
      "Epoch 270: Loss 0.18158849416695616\n",
      "Epoch 271: Loss 0.18158358651291112\n",
      "Epoch 272: Loss 0.18157862831894736\n",
      "Epoch 273: Loss 0.1815738071568579\n",
      "Epoch 274: Loss 0.18156895850796276\n",
      "Epoch 275: Loss 0.1815642216466663\n",
      "Epoch 276: Loss 0.18155935386098962\n",
      "Epoch 277: Loss 0.1815545045084774\n",
      "Epoch 278: Loss 0.18154984619207726\n",
      "Epoch 279: Loss 0.18154537412037505\n",
      "lr 0.0005233476330273611\n",
      "Epoch 280: Loss 0.1815406311848323\n",
      "Epoch 281: Loss 0.18153601131659325\n",
      "Epoch 282: Loss 0.18153135167018028\n",
      "Epoch 283: Loss 0.18152665587971117\n",
      "Epoch 284: Loss 0.1815220507163848\n",
      "Epoch 285: Loss 0.18151751580976444\n",
      "Epoch 286: Loss 0.18151291179487547\n",
      "Epoch 287: Loss 0.18150838853505624\n",
      "Epoch 288: Loss 0.1815040425917074\n",
      "Epoch 289: Loss 0.18149966430087783\n",
      "lr 0.000471012869724625\n",
      "Epoch 290: Loss 0.18149515234495422\n",
      "Epoch 291: Loss 0.18149064624540448\n",
      "Epoch 292: Loss 0.181486144400249\n",
      "Epoch 293: Loss 0.1814817933674128\n",
      "Epoch 294: Loss 0.1814775454378167\n",
      "Epoch 295: Loss 0.18147324437553952\n",
      "Epoch 296: Loss 0.18146898691484142\n",
      "Epoch 297: Loss 0.18146474893438622\n",
      "Epoch 298: Loss 0.18146041533744572\n",
      "Epoch 299: Loss 0.1814562692039064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 16)\n",
    "        self.fc3 = torch.nn.Linear(16, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        fc1_op = torch.nn.functional.relu(self.fc1(x))\n",
    "        fc2_op = torch.nn.functional.relu(self.fc2(fc1_op))\n",
    "        return self.fc3(fc2_op)\n",
    "\n",
    "def train(x, y, model, epochs=1, bs=32, lr=1e-3):\n",
    "    iters = len(x)//bs\n",
    "    x, y = torch.FloatTensor(x.astype('float32').values), torch.FloatTensor(y.values)\n",
    "#     optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    floss = torch.nn.BCEWithLogitsLoss()\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        if e % 10 == 0 and e > 0:\n",
    "            lr *= 0.9\n",
    "            print ('lr', lr)\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        ind = np.random.permutation(range(len(x)))\n",
    "        x, y = x[ind], y[ind]\n",
    "        for i in range(iters):\n",
    "            batch_x, batch_y = x[i*bs:(i+1)*bs], y[i*bs:(i+1)*bs]\n",
    "            optim.zero_grad()\n",
    "            loss = floss(model(batch_x), batch_y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(f'Epoch {e}: Loss {np.mean(losses)}')\n",
    "\n",
    "def predict(x, model):\n",
    "    x = torch.FloatTensor(x.astype('float32').values)\n",
    "    return torch.nn.functional.sigmoid(model(x)).detach().numpy()[:, 0]\n",
    "\n",
    "df_train = pd.read_csv('train.csv', encoding=\"ISO-8859-1\")\n",
    "df_train['train'] = 1\n",
    "df_test = pd.read_csv('test.csv', encoding=\"ISO-8859-1\")\n",
    "df_test['train'] = 0\n",
    "df_all = df_train.append(df_test)\n",
    "aus_mean = df_all.application_underwriting_score.mean()\n",
    "df_all.fillna({'application_underwriting_score': aus_mean}, inplace=True)\n",
    "df_all.fillna({'Count_3-6_months_late': 0, 'Count_6-12_months_late': 0, \\\n",
    "                     'Count_more_than_12_months_late': 0}, inplace=True)\n",
    "\n",
    "print(df_test.shape, df_train.shape, df_all.shape)\n",
    "\n",
    "OHE_columns = [x for x in df_all.columns if x == 'sourcing_channel']\n",
    "df_all = pd.get_dummies(df_all, columns= OHE_columns)\n",
    "df_all.residence_area_type = pd.factorize(df_all.residence_area_type)[0]\n",
    "df_all['amount_paid'] = df_all.no_of_premiums_paid * df_all.premium\n",
    "df_train = df_all[df_all.train == 1]\n",
    "df_train.reset_index(drop = True, inplace=True)\n",
    "df_test = df_all[df_all.train == 0].reset_index(drop = True)\n",
    "df_test.reset_index(drop = True, inplace=True)\n",
    "print(df_test.shape, df_train.shape, df_all.shape)\n",
    "\n",
    "np.random.seed(1)\n",
    "train_rows = np.random.choice(df_train.index, int(len(df_train)* 0.8), replace=False)\n",
    "valid_rows = [x for x in df_train.index if x not in train_rows]\n",
    "df_train1 = df_train.loc[train_rows]\n",
    "df_valid1 = df_train.loc[valid_rows]\n",
    "\n",
    "train_X = df_train1.drop(['id', 'train', 'renewal'], axis = 1)\n",
    "scaler = MinMaxScaler()\n",
    "train_X = pd.DataFrame(scaler.fit_transform(train_X), columns = train_X.columns)\n",
    "train_Y = df_train1[['renewal']]\n",
    "valid_X = df_valid1.drop(['id', 'train', 'renewal'], axis = 1)\n",
    "scaler = MinMaxScaler()\n",
    "valid_X = pd.DataFrame(scaler.fit_transform(valid_X), columns = valid_X.columns)\n",
    "valid_Y = df_valid1[['renewal']]\n",
    "test_X = df_test.drop(['id', 'train', 'renewal'], axis = 1)\n",
    "test_X = pd.DataFrame(scaler.fit_transform(test_X), columns = test_X.columns)\n",
    "\n",
    "print(train_X.shape, train_Y.shape, test_X.shape, valid_X.shape, train_X.Income.mean())\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "model = MyModel(train_X.shape[1])\n",
    "\n",
    "train(train_X, train_Y, model, epochs=300, lr = 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.8403989259006422\n",
      "val auc: 0.8287483643203657\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict(train_X, model)\n",
    "print(f'train auc: {roc_auc_score(train_Y, train_pred)}')\n",
    "\n",
    "val_pred = predict(valid_X, model)\n",
    "print (f'val auc: {roc_auc_score(valid_Y, val_pred)}')\n",
    "test_pred = predict(test_X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premium</th>\n",
       "      <th>renewal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649</td>\n",
       "      <td>3300</td>\n",
       "      <td>0.996194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81136</td>\n",
       "      <td>11700</td>\n",
       "      <td>0.987653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  premium   renewal\n",
       "0    649     3300  0.996194\n",
       "1  81136    11700  0.987653"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = df_test[['id', 'premium']]\n",
    "submission['renewal'] = test_pred\n",
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premium</th>\n",
       "      <th>renewal</th>\n",
       "      <th>incentives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649</td>\n",
       "      <td>3300</td>\n",
       "      <td>0.996194</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81136</td>\n",
       "      <td>11700</td>\n",
       "      <td>0.987653</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70762</td>\n",
       "      <td>11700</td>\n",
       "      <td>0.939059</td>\n",
       "      <td>431.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53935</td>\n",
       "      <td>5400</td>\n",
       "      <td>0.977605</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15476</td>\n",
       "      <td>9600</td>\n",
       "      <td>0.938666</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  premium   renewal  incentives\n",
       "0    649     3300  0.996194       192.0\n",
       "1  81136    11700  0.987653       443.0\n",
       "2  70762    11700  0.939059       431.0\n",
       "3  53935     5400  0.977605       271.0\n",
       "4  15476     9600  0.938666       387.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_per = lambda x: 20*(1-np.exp(-2*(1-np.exp(-x/400))))\n",
    "inc_rev = lambda p, pr, inc: p*pr*imp_per(inc)/100. - inc\n",
    "def opt_inc(p, pr):\n",
    "    if inc_rev(p, pr, 1)<0: return 0\n",
    "    max_rev = 0\n",
    "    max_inc = 0\n",
    "    inc = 1\n",
    "    while inc_rev(p, pr, inc) > max_rev:\n",
    "        max_rev = inc_rev(p, pr, inc)\n",
    "        max_inc = inc #16\n",
    "        inc *= 2 #32\n",
    "    fac = inc / 4. #8\n",
    "    inc -= fac #24\n",
    "    fac /= 2. #4\n",
    "    while fac>=1:\n",
    "        if inc_rev(p, pr, inc)>max_rev:\n",
    "            max_rev = inc_rev(p, pr, inc)\n",
    "            max_inc = inc\n",
    "            inc += fac\n",
    "        else:\n",
    "            inc -= fac\n",
    "        fac -= 1.\n",
    "    return max_inc\n",
    "\n",
    "submission['incentives'] = submission.apply(lambda x: opt_inc(x.renewal, x.premium), axis=1)\n",
    "# final_df1.incentives = [0 if x <= 0 else x for x in final_df1.incentives]\n",
    "submission.drop('premium', axis = 1).to_csv('NN_8287_binarysearch_modified.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
